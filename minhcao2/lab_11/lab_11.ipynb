{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STAT 207 Lab 11: Regularized Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due: Wednesday, April 29, 23:59:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Michael Cao AND minhcao2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab concerns uses regularized logistic regression to develop a model for classifying email spam. The data are in the file \"spam100.csv\". The target variable 'spam' is 1 if the message is labeled as spam, and 0 if the message is labeled as not spam.\n",
    "\n",
    "These data comprise a subset of a larger data set of email messages and numerous quantitative measures of the messages themselves. \n",
    "\n",
    "In the provided data for this lab, all of the quantitative feature variables have been scaled to  a 0-100 range by dividing by the maximum observed value for each feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Data Preparation and visualization (10 pts)\n",
    "\n",
    "**a) (3 pts)** Read the email spam data into a data frame, and separate out a feature matrix (data frame) X, and a target array y. Display the dimensions of X and its first few rows, including column names. Also display the first few elements of y along with its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_will</th>\n",
       "      <th>word_freq_people</th>\n",
       "      <th>word_freq_report</th>\n",
       "      <th>word_freq_money</th>\n",
       "      <th>word_freq_mail.1</th>\n",
       "      <th>word_freq_email</th>\n",
       "      <th>word_freq_free</th>\n",
       "      <th>word_freq_free.1</th>\n",
       "      <th>word_freq_your</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.639752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.441809</td>\n",
       "      <td>1.614274</td>\n",
       "      <td>2.344197</td>\n",
       "      <td>56.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.453222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.940594</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.857143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493996</td>\n",
       "      <td>0.509771</td>\n",
       "      <td>1.572327</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>26.007802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.001100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.428571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.536249</td>\n",
       "      <td>2.124044</td>\n",
       "      <td>1.629503</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>16.644993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.219219</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.517052</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.244306</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.442105</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>7.632933</td>\n",
       "      <td>3.125</td>\n",
       "      <td>...</td>\n",
       "      <td>3.250975</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.801802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.180418</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>15.217391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.370645</td>\n",
       "      <td>0.169924</td>\n",
       "      <td>0.428816</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>49.482402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.067457</td>\n",
       "      <td>0.934579</td>\n",
       "      <td>1.029160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.438844</td>\n",
       "      <td>0.509771</td>\n",
       "      <td>1.057747</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>2.56469</td>\n",
       "      <td>10.455487</td>\n",
       "      <td>7.891637</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.924240</td>\n",
       "      <td>4.418012</td>\n",
       "      <td>37.249857</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.950585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.650165</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.519669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.573165</td>\n",
       "      <td>0.594732</td>\n",
       "      <td>1.658090</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>47.204161</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.768569</td>\n",
       "      <td>2.718777</td>\n",
       "      <td>2.001144</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>7.022107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.108108</td>\n",
       "      <td>5.314961</td>\n",
       "      <td>5.314961</td>\n",
       "      <td>23.982398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     char_freq_;  char_freq_(  char_freq_[  char_freq_!  char_freq_$  \\\n",
       "0        0.00000     2.639752     0.000000     0.000000     0.000000   \n",
       "1        0.00000    17.857143     0.000000     0.000000     0.000000   \n",
       "2        0.00000     0.000000     0.000000     0.000000    19.428571   \n",
       "3        0.00000     4.244306     0.000000     0.460087     0.000000   \n",
       "4        0.00000    15.217391     0.000000     0.000000     0.000000   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "395      0.00000    49.482402     0.000000     0.000000     0.000000   \n",
       "396      0.00000     0.000000     0.000000     0.000000     0.000000   \n",
       "397      2.56469    10.455487     7.891637     0.000000     1.333333   \n",
       "398      0.00000     0.000000     0.000000     3.519669     0.000000   \n",
       "399      0.00000     0.000000     0.000000     0.000000    16.285714   \n",
       "\n",
       "     char_freq_#  capital_run_length_average  capital_run_length_longest  \\\n",
       "0            0.0                    0.441809                    1.614274   \n",
       "1            0.0                    0.493996                    0.509771   \n",
       "2            0.0                    1.536249                    2.124044   \n",
       "3            0.0                    0.442105                    0.934579   \n",
       "4            0.0                    0.370645                    0.169924   \n",
       "..           ...                         ...                         ...   \n",
       "395          0.0                    1.067457                    0.934579   \n",
       "396          0.0                    0.438844                    0.509771   \n",
       "397          0.0                    0.924240                    4.418012   \n",
       "398          0.0                    0.573165                    0.594732   \n",
       "399          0.0                    0.768569                    2.718777   \n",
       "\n",
       "     capital_run_length_total  word_freq_our  ...  word_freq_will  \\\n",
       "0                    2.344197         56.250  ...        0.000000   \n",
       "1                    1.572327          0.000  ...       26.007802   \n",
       "2                    1.629503          0.000  ...       16.644993   \n",
       "3                    7.632933          3.125  ...        3.250975   \n",
       "4                    0.428816          0.000  ...        0.000000   \n",
       "..                        ...            ...  ...             ...   \n",
       "395                  1.029160          0.000  ...        0.000000   \n",
       "396                  1.057747          0.000  ...        0.000000   \n",
       "397                 37.249857          0.000  ...        1.950585   \n",
       "398                  1.658090          0.000  ...       47.204161   \n",
       "399                  2.001144          0.000  ...        7.022107   \n",
       "\n",
       "     word_freq_people  word_freq_report  word_freq_money  word_freq_mail.1  \\\n",
       "0                 0.0               0.0         0.000000         22.453222   \n",
       "1                 0.0               0.0         0.000000          0.000000   \n",
       "2                 0.0               0.0        19.219219          0.000000   \n",
       "3                 0.0               0.0         0.000000          0.000000   \n",
       "4                 0.0               0.0         0.000000          0.000000   \n",
       "..                ...               ...              ...               ...   \n",
       "395               0.0               0.0         0.000000          0.000000   \n",
       "396               0.0               0.0         0.000000          0.000000   \n",
       "397               0.0               0.0         0.000000          0.000000   \n",
       "398               0.0               0.0         0.000000          0.000000   \n",
       "399               0.0               0.0         0.000000          0.000000   \n",
       "\n",
       "     word_freq_email  word_freq_free  word_freq_free.1  word_freq_your  spam  \n",
       "0           0.000000        0.000000          0.000000        5.940594     1  \n",
       "1           0.000000        0.000000          0.000000       11.001100     0  \n",
       "2           0.000000        0.000000          0.000000       70.517052     1  \n",
       "3           1.801802        0.000000          0.000000        4.180418     0  \n",
       "4           0.000000        0.000000          0.000000        0.000000     0  \n",
       "..               ...             ...               ...             ...   ...  \n",
       "395         0.000000        0.000000          0.000000        0.000000     0  \n",
       "396         0.000000        0.000000          0.000000        0.000000     0  \n",
       "397         0.750751        0.000000          0.000000        1.650165     0  \n",
       "398         0.000000        0.000000          0.000000        0.000000     0  \n",
       "399         8.108108        5.314961          5.314961       23.982398     1  \n",
       "\n",
       "[400 rows x 26 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn import metrics\n",
    "df = pd.read_csv(\"spam100.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['spam']\n",
    "X = df.drop(columns='spam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 25)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.63391783,  8.74344743,  2.22639902,  2.38168656,  1.22107396,\n",
       "         0.1329848 ,  0.70764727,  1.50545021,  5.68166612,  3.30341312,\n",
       "         1.77682209,  0.06842253,  0.72828189,  3.24227865,  2.28601761,\n",
       "         1.15957447,  6.85111917,  2.57101482,  0.45503308,  0.21468277,\n",
       "         2.28601761,  0.98524056,  0.40961635,  0.40961635,  4.69302249],\n",
       "       [ 0.41287619,  5.7233829 ,  1.12503123, 10.54715547, 14.81558442,\n",
       "         0.97780301,  2.95724689,  7.71092402, 13.0363671 , 12.75410354,\n",
       "         8.620245  ,  5.65478574,  7.26793628, 12.6686217 ,  7.76412776,\n",
       "         6.55757576,  7.58876148,  7.7899413 ,  2.11739241,  4.24788425,\n",
       "         7.76412776,  5.63563564,  4.98031496,  4.98031496, 15.55422209]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_matrix = np.array((X[y==0].mean(), X[y==1].mean()))\n",
    "display(mean_matrix.shape, mean_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      0\n",
       "2      1\n",
       "3      0\n",
       "4      0\n",
       "      ..\n",
       "395    0\n",
       "396    0\n",
       "397    0\n",
       "398    0\n",
       "399    1\n",
       "Name: spam, Length: 400, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.5875\n",
       "1    0.4125\n",
       "Name: spam, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "props = y.value_counts(normalize =  True)\n",
    "props"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) (1pt)** Compute the proportion of spam messages in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfspam = df[ df[\"spam\"] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnonspam = df[ df[\"spam\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[ df[\"spam\"] == 1]) / len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) (2 pts)** Show a bar plot comparing the proportion of zeros (non-spam) and the proportion of ones (spam) in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x280e364d108>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "sns.barplot(x = props.index, y = props)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) (3 pts)** Create a heat map comparing the mean values for all the feature variables in X when y=0 versus their mean values when y=1.  For an example see the notes, \"15_regularized_logit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWgAAAEZCAYAAAC6m7+xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAeBklEQVR4nO3debxcRZ338c8XwmpCCBAIECCKAq6gAiqiMIASUQw6MAoqi4xhxhGQxyU4+Aw64yA+OG4PbteFnSCCIpssgsigLAFMQiBRFAKJrBLZYSS5v/nj1JXDSd/u0zfdt6uT7/u+6nXPUl1VvZxfV9fZFBGYmVl+Vut1A8zMrDEHaDOzTDlAm5llygHazCxTDtBmZplygDYzy5QDtK3SJJ0q6Qu9bodZIw7QGZG0UNIzkp4spc1WsMzdJS3uVBtr1nmqpJD07sryr6Xlh45me0aLCh+TNFfS05IekHSNpPf3um3Wnxyg87NvRIwtpft62RhJY0b40N8Dh1TKOQD4YyfalalvAB8HPgFsCGwOfBaY2ihzCujeBm1Y/nD0CUlvlPQbSY9KmiNp99K6wyTNl/SEpLskHZGWvwj4ObBZuUde/Vlf7WWnnvwMSXOBpySNSY87X9LDku6WdFSLJl8EvFnShDQ/FZgLPFB5Xh9Obf+LpMslbVVa93VJiyQ9LukWSW8prfucpHMlnZ6e9+2SdiytnyHpT2nd7yTt2aStG0m6MuX91VAbJH1T0n9V2nuRpI9XC5C0DfBR4P0RcWVEPBMRyyLiuog4tJTvGkn/KenXwNPAS9Jre6GkJZL+IOkjpfx13qvPSLojvYanSFq7yXO1PuIA3QckbQ5cAnwB2AD4JHC+pIkpy0PAu4D1gMOAr0p6XUQ8BbwDuG8EPfIDgXcC6wODFAF3DkWvcE/g45L2bvL4Z4ELgaGf9wcDp1ee137AvwLvBSYC/w3MLGWZBeyQnvPZwI8rwefdwDmpjRcCJ6dytwU+BuwUEeOAvYGFTdr6AeA/gI2A2cBZaflpwIFDvVxJG6XnPrNBGXsAiyLi5ib1DPkQMB0YB9yTylsMbAbsD5zQ4gulUfv3BrYGtqHotdtKwAE6PxekXvKjki5Iyz4IXBoRl0bEYERcCdwM7AMQEZdExB+j8CvgCuAtjYuv7RsRsSgingF2AiZGxL9HxF8j4i7gezwffIdzOnCwpPHAbsAFlfVHAF+MiPkRsRQ4AdhhqAcbEWdGxCMRsTQi/gtYC9i29Pjr0muyDDgD2D4tX5byvkLSGhGxMCKaDa1cEhHXRsT/AMcBb5K0RUTcBDxGEZRJz/eaiHiwQRkbsfyvg8XpfXy2/MsAODUibk/PeRKwKzAjIp6NiNnA9ymCeF0np/dqCfCfFF+uthJwgM7PfhGxfkr7pWVbAQeUAvejFBv1pgCS3iHphvQT+VGKwL3RCrZjUWl6K4phknL9/wps0qyAiLiOomf8WeDiFOzLtgK+XipzCSCKXjqSPpGGPx5L68dXnlc5ID4NrC1pTET8gWIs+HPAQ5LOUfOdrX97rhHxZGrHUP7TKL4gSf/PGKaMR0jvR6msyam9a6XntVx9qZ4lEfFEadk9pNegpnJ595Tabn3OAbo/LALOKAXu9SPiRRFxoqS1gPOBLwObRMT6wKU8HxAaXa7wKWDd0vykBnnKj1sE3F2pf1xE7FOj7WdS7DQ7vcG6RcARlXLXiYjfpPHmGcA/ABPS83qMFwa6YUXE2RGxK8WXQABfapJ9i6EJSWMphlSGhoLOBKZJ2h54Ocv/ChhyNTC5PA7erHml6fuADSSNKy3bEvhTmq7zXm1Rmt6y1Hbrcw7Q/eFMYF9Je0taXdLaaWfRZGBNih7aw8BSSe8A3l567IPAhmmYYchsYB9JG0iaRNHbbOYm4PG0422d1IZXSdqpRtu/AbwNuLbBuu8An5H0SgBJ4yUdkNaNA5am5zVG0r9RjLG3JGlbSXukL69ngWcohj2Gs4+kXSWtSTEWfWNELAKIiMUUY+FnAOc3+BVAyvc74LvAOZLeNvQ6Abs0a2uq5zfAF9P7+hrgcJ4fB6/zXv2LpMmSNqD4ZfOjZnVa/3CA7gNpI55GsfE9TNHz/BSwWvppfBRwLvAX4CCKHWZDj11AsRPqrjSUsBlFsJlDsePsClps0GmMd1+KHXZ3A3+mGCcd3+xx6bFLIuKqaHDh8Yj4KUXP9hxJjwPzKHZqAlxOcQTK7yl+tj/LC3/KN7MWcGJq5wPAxhSv3XDOBo6nGNp4PcVOt7LTgFcz/PDGkH+h+EL6SiprMUXAfx9wb5PHHQhMoej5/hQ4Pu1ngHrv1dlp3V0p+cSblYR8wX6z5iS9leJXzJSIGOx1e8okLQT+MSJ+0eu2WOe5B23WhKQ1gKOB7+cWnG3l5wBtNgxJLwcepTg642s9bo6tgjzEYWaWKfegzcwy5QBtZpapkV6prOvW2fmTtcZe3nDQe2uXeeNF19TOO/OrH66dd7eXTmydCdjsnSfULnPslG1q5z1g31fXynfUm6bULnPpYP39YT+544HWmZKr5jU6S3p5L9m01iHPAHxi1xfXzvvtWfWO1Lt/ydO1y5w4vv61icasXr9PdMqZ19fKt8VL6590uNcbt6ydd97CJbXyPfXUX2uX2Y7rjv272nnXHlPvBKZmnl3a8KSurtVXR7YB2sxsNOW4O84B2swMiPodaGpecWCFOUCbmUHjq9b0mAO0mRkw6ABtZpYnD3GYmeXKPWgzszxlGJ8doM3MwIfZmZllazDDCO1Tvc3MMuUetJkZHuIwM8tWe4fZjQ4HaDMz3IM2M8tWhvHZAdrMDCDHu0s5QJuZ4SEOM7NsZRiffRy0mRlQROi6qQVJP5T0kKR5DdZ9UlJI2qhVOQ7QZmYUh9nV/avhVGBqdaGkLYC3AffWKcQB2syM4nrQdVMrEXEt0Oimjl8FPk3NERUHaDMzip2EdZOk6ZJuLqXprcqX9G7gTxExp26bvJPQzAxoZzdhRAwAA3XzS1oXOA54ezstcg/azIz2etAjsDXwYmCOpIXAZOBWSZOaPcg9aDMzunuYXUTcBmw8NJ+C9I4R8edmj3MP2syMzvagJc0Erge2lbRY0uEjaZN70GZmdPZU74g4sMX6KXXKcYA2MyPPMwkdoM3M8LU4zMyy5Qv2m5nlKr/47ABtZgZZxmcHaDMzgMEMB6EdoM3MIMsutAO0mRlZxmcHaDMz8GF2ZmbZ8mF2ZmaZcg/azCxTDtBmZpnyEIeZWa7yi88O0GZmkGV8doA2MwOPQZuZZauTF+zvFAdoMzM8xGFmlq0MO9C+aayZGRSH2dX9a0XSDyU9JGleadlJkhZImivpp5LWb1WOA7SZGRRjHHVTa6cCUyvLrgReFRGvAX4PfKZVIQ7QZmZ0Nj5HxLXAksqyKyJiaZq9AZjcqhyPQZuZMeoX7P8w8KNWmdyDNjODtrrQkqZLurmUptetRtJxwFLgrFZ53YM2M6O9w+wiYgAYaLcOSYcA7wL2jBoHXjtAm5nR/cPsJE0FZgC7RcTTdR4z6kMckg4b7TrNzFrp8GF2M4HrgW0lLZZ0OHAyMA64UtJsSd9pVU4vetCfB05ptCKN40wHGLPV2xiz8WtGs11mtgrrZA86Ig5ssPgH7ZbTlQAtae5wq4BNhntceVxnnZ0/meF5PWa2shrlozhq6VYPehNgb+AvleUCftOlOs3MRiy/8Ny9AH0xMDYiZldXSLqmS3WamY1Yhh3o7gToiDi8ybqDulGnmdmK8C2vzMwyNZhffHaANjODVWsnoZlZX/EQh5lZpjzEYWaWKfegzcwyleEQtAO0mRk4QJuZZWtZhhHaAdrMDI9Bm5llK8MOtAO0mRnAoHvQZmZ5cg/azCxTPtXbzCxTg71uQAMO0GZmQI2bbI+6Ub9prJlZjqKN1IqkH0p6SNK80rINJF0p6c70f0KrchygzcwoetB1Uw2nAlMry44FroqIlwFXpfmmHKDNzCjGoOumViLiWmBJZfE04LQ0fRqwX6tyPAZtZgYMdv96o5tExP0AEXG/pI1bPcA9aDMz2huDljRd0s2lNL0bbXIP2syM9o6DjogBYKDNKh6UtGnqPW8KPNTqAe5Bm5nR2aM4hnEhcEiaPgT4WasHuAdtZkZnj4OWNBPYHdhI0mLgeOBE4FxJhwP3Age0KscB2swMWNbBfYQRceAwq/ZspxwHaDMzfC0OM7NsZRifHaDNzMA9aDOzbHX/PJX2OUCbmeEhDjOzbPmu3mZmmfIQh5lZpryT0MwsUxnGZwdoMzPwEIeZWbZyvCehA7SZGbA0w9t6O0CbmQGxIhcS7ZJa14OW9BJJF0n6c7pT7c8kvaTbjTMzGy2DUT+NlroX7D8bOBeYBGwG/BiY2a1GmZmNtoj6abTUDdCKiDMiYmlKZ7JCNxYwM8vLYETtNFrqjkH/UtKxwDkUgfl9wCWSNgCIiOrtxc3M+sqyPt5J+L70/4jK8g9TBGyPR5tZX+vbMwkj4sXdboiZWS9lGJ/rBWhJBzdaHhGnd7Y5Zma90ckRDknHAP9IMcJwG3BYRDzbbjl1hzh2Kk2vTXHjw1sBB2gzWyl0aohD0ubAUcArIuIZSecC7wdObbesukMcR1YaMB44o93KzMxy1eEhjjHAOpKeA9YF7htpISPxNPCyET7WzCw7yzp0BkpE/EnSl4F7gWeAKyLiipGUVXcM+iKeP+55deDlFCeumJmtFNqJz5KmA9NLiwYiYiCtmwBMA14MPAr8WNIH0/kjbanbg/5yaXopcE9ELG63MjOzXLUToFMwHhhm9V7A3RHxMICknwC7AG0H6FpnEkbEr4AFwDhgAvDXdisyM8tZRNROLdwLvFHSupJEcVDF/JG0qe4Qxz8AJwHXAAL+v6RPRcR5I6m0lolb1sr2tldvXLvIWb/etHbeg0+oP2T03CP318v42MO1y3zy9/WPyDnle/X2P9x+z861y7xzwYO18/7l/vp5WXR7rWw3rTu+dpGXXf262nmfePSJWvmWLXmgdpnTPrR37byXXVHv+QPE4nrb9L0L59Qu87S7X1s777KHF9XL+NRjtctsxxueq3/g25zP77nC9XXqMLuIuFHSeRRHui0Ffsvwve2m6g5xHAfsFBEPAUiaCPwC6F6ANjMbRZ28YH9EHA8cv6Ll1A3Qqw0F5+QR6l9oycwse506iqOT6gboyyRdzvOXGH0fcGl3mmRmNvr69lTviPiUpPcCu1KMQQ9ExE+72jIzs1HUl/cklLQ6cHlE7AX8pPtNMjMbfRnG59YBOiKWSXpa0viI6M7uWjOzHuvby40CzwK3SboSeGpoYUQc1ZVWmZmNsn4O0JekBM+f8q3ON8fMrDcyPIijeYCWNA2YHBHfTPM3ARMpgvSM7jfPzGx05LiTsNWxzJ8GLizNrwm8Htgd+KcutcnMbNTleFfvVkMca0ZE+XzP69INYpdIelEX22VmNqpy7EG3CtATyjMR8bHS7MTON8fMrDdyHINuNcRxo6SPVBdKOgK4qTtNMjMbfYMRtdNoadWDPga4QNJBFFdmgmIMei1gv242zMxsNPXdYXbpAkm7SNoDeGVafElEXN31lpmZjaIM43Pta3FcDTgom9lKqx93EpqZrRIyjM8O0GZm0Idj0GZmq4rBDI+zc4A2M6M/j4M2M1slRBt/rUhaX9J5khZImi/pTSNpk3vQZmZ0fCfh14HLImJ/SWsC646kEAdoMzM6d5idpPWAtwKHpnL/Cvx1JGU5QJuZ0dG7er8EeBg4RdL2wC3A0RHxVPOHLc9j0GZmtHe5UUnTJd1cStNLRY0BXgd8OyJeS3EXqmNH0ib3oM3MaO846IgYAAaGWb0YWBwRN6b58xhhgHYP2syMzl2wPyIeABZJ2jYt2hO4YyRtcg/azIyOX4vjSOCsdATHXcBhIynEAdrMjM4eZhcRs4EdV7QcB2gzM3yqt5lZtny5UTOzTDlAm5llKsP47ABtZgbuQZuZZSvD+OwAbWYGPorDzCxbHuIwM8tUhvHZAdrMDNyDNjPLVobx2QHazAy8k9DMLFse4jAzy1SG8dkB2swM3IM2M8uWA7SZWaYyjM8O0GZm4KM4zMyylWOA9l29zczo3F29h0haXdJvJV080ja5B21mRld2Eh4NzAfWG2kB7kGbmdHZHrSkycA7ge+vSJu61oOWtB0wDdgcCOA+4MKImN+tOs3MRqrDY9BfAz4NjFuRQrrSg5Y0AzgHEHATMCtNz5R0bDfqNDNbERFRO0maLunmUpo+VI6kdwEPRcQtK9qmbvWgDwdeGRHPlRdK+gpwO3BiowelJzkdYMyr3seYLXfpUvPMzF6onTHoiBgABoZZ/Wbg3ZL2AdYG1pN0ZkR8sN02dWsMehDYrMHyTdO6hiJiICJ2jIgdHZzNbFRFG6lZMRGfiYjJETEFeD9w9UiCM3SvB/1x4CpJdwKL0rItgZcCH+tSnWZmI7bKnOodEZdJ2gbYmWInoYDFwKyIWNaNOs3MVkQ3AnREXANcM9LHd+0ojogYBG7oVvlmZp00ODjs6GvP+EQVMzNoObbcCw7QZmasQmPQZmb9xgHazCxTDtBmZrnKLz47QJuZgY/iMDPLloc4zMwy5QBtZpar/OKzA7SZGbgHbWaWLe8kNDPLVX4daAdoMzPwEIeZWbYcoM3MMuUAbWaWKQdoM7NMxaADtJlZltyDNjPLVI4BerVeN8DMLAcRUTs1I2kLSb+UNF/S7ZKOHmmb3IM2M4NOnqiyFPhERNwqaRxwi6QrI+KOdgtygDYzo3NDHBFxP3B/mn5C0nxgc8AB2sxsJKKNa3FImg5MLy0aiIiBBvmmAK8FbhxJmxygzcwAon6ATsF4uYBcJmkscD7w8Yh4fCRNcoA2MwPo4FEcktagCM5nRcRPRlqOA7SZGbTVg25GkoAfAPMj4isrUpYPszMzg6IHXTc192bgQ8AekmantM9ImuQetJkZwODSjhQTEdcB6kRZDtBmZtDRMehOcYA2M4OOjUF3kgO0mRm4B21mli33oM3MMuUetJlZpgaX9boFy3GANjMDD3GYmWXLQxxmZplyD9rMLFO+aayZWabcgzYzy5SP4jAzy5R70GZmmfJRHGZmmXIP2swsU+5Bm5llyjsJzcwy5SEOM7NMeYjDzCxT7kGbmWUqwx40EdFXCZjeyXzdytvr+vuprb2uv5/a2uv6+62t/Z563oC2Gww3dzJft/L2uv5+amuv6++ntva6/n5ra7+n1Zr3r83MrFccoM3MMtWPAXqgw/m6lbfX9beTd1Wvv528q3r97eTtdf19T2lMx8zMMtOPPWgzs1WCA7SZWaYcoM3MMpX1mYSStgOmAZsDAdwHXBgR8ztQ7ubAjRHxZGn51Ii4rDS/MxARMUvSK4CpwIKIuLRGHadHxME18u0K7AzMi4grKuveAMyPiMclrQMcC7wOuAM4ISIeS/mOAn4aEYtq1Lcm8H7gvoj4haSDgF2A+cBARDxXyb818B5gC2ApcCcwc6hus9EkaeOIeKjX7Rgt2fagJc0AzgEE3ATMStMzJR3bRjmHVeaPAn4GHAnMkzSttPqEUr7jgW8A35b0ReBkYCxwrKTjKmVeWEkXAe8dmq/kvak0/ZFU7jjg+AbP64fA02n668B44Etp2SmlfP8B3CjpvyV9VNLEJi/JKcA7gaMlnQEcANwI7AR8v8Fr9R1g7bR+HYpAfb2k3ZvU0XckbdyFMjfsdJmdIGm8pBMlLZD0SErz07L1a5bx88r8epK+KOmM9KVfXvetyvwkSd+W9E1JG0r6nKTbJJ0radNSvg0qaUPgJkkTJG2wAi9B/+j1mTJNzhb6PbBGg+VrAne2Uc69lfnbgLFpegpwM3B0mv9tJd/qwLrA48B6afk6wNxKmbcCZwK7A7ul//en6d0qect1zAImpukXAbdV8s4v11FZN7tcJsWX7duBHwAPA5cBhwDjKo+bm/6PAR4EVk/zavC8biutXxe4Jk1vWX4eadl44ERgAfBISvPTsvXbeL9+XppeD/gicAZwUCXftyrzk4BvA98ENgQ+l9p/LrBpJe8GlbQhsBCYAGxQyje18vx+AMwFzgY2qZR5IrBRmt4RuAv4A3BPg8/ArcBnga1rvB47Ar9Mn68tgCuBx9Jn57WlfGOBfwduT+sfBm4ADm1Q5uXADGBS5fWbAVxZWva6YdLrgfsrZZ6fXoP9gAvT/FrDfHYvo+ggHZtezxnpM3Uk8LNSvkHg7kp6Lv2/ayRxpd9SzxvQ5IO5ANiqwfKtgN9Vls0dJt0G/E8l7x2V+bHpA/MVKkGv0XSan12ZXw04Jm08O6RlDT9AwJwUCDakcspqg3p+DByWpk8BdkzT2wCzSvmqG8AawLuBmcDDlXXzKL7kJgBPkAISRS95fiXvbaWNbAJwS7mcSt5aG31aXmvD78ZGn/LW2vDLdVD8uvhC+vwdA1xQfa1K078Ediq9V9X3+W7gy8C9FL8OjwE2G+bzchPwDuBAYBGwf1q+J3B9Kd/PgEOBycD/Af4v8DLgNIrhsHKZv2tUV3UdsAy4Oj2fanqmxTZxHPBris959b0qb1vVDlR5G/xkel9fXX7tmsWNlS31vAFNPihTKXogP6c4MH0gvVl/oNSzSXkfBHZIG085TaEYay3nvZoUREvLxgCnA8tKy24E1k3Tq5WWj69+4ErrJlME1ZOrH7xSnoUUvau70/9JafnYBh/y8cCpwB9Te55Lj/kVsH0p328b1ZXWrVOZPyaVcQ9wFHAV8D2KYHx8Je/RFMFugOILc+jLYiJwbSVvrY0+zdfa8Lux0af5Whs+LwzQ1TKq8wuAMWn6hsq66i+jcrlvAb4FPJCe//RK3mbPq7xuTmXdrKHPLsV+k/K6K4BPU/oVAGxC8aX2i9KyecDLhnlPF1Xm51PaTtKyQyh69PdUls8pTX+hxWs1tE19hWIocJXoOf/t+fe6AU0bV3y43gj8PbB/ml69Qb4fALsOU8bZDd7wScPkfXNpeq1h8mxU3rCHyfNOKr2WGs91XeDFw6wbB2xP0cPcpMH6bdqsazNSjw1YP722Ow+T95Vp/XYtyqy10afltTb8bm30pc9B0w0fWEzRG/0ExZeaSuuqw0FHptdgD4rhla8BbwU+D5xRybvcFzzFcNpU4JTK8usphq4OoPhS3S8t341Szxz4zdA2AOwLXF5aV/2CnECxL2MB8BdgSXqtv8QLh3j2B7Yd5n3arzL//4C9GuSbSmVIkmIoZmyDvC8Fzhumvn0phmweaOez3u+p5w1wWjlSZaNfUtnoJ1Ty1trwu73Rp/XDbvjA8ZU0tL9gEnB6g/y7Az+i2CdwG3ApMJ3Usy7lO6eN13V7iuGjnwPbUewsfpTiS2qXUr7XUAyHPApcR/rSpvi1c1SDcrcD9qq+Ziz/63Q7iuGUpvla5H1HG3mHrZ9i/8+rhqt/ZUw9b4DTyp9IQyOdzNvJMisb/qjXP9qvFcXQ1u+ACyiG3KaV1t3abr40f2Sn87ZT/8qaet4Ap5U/Mcx4/Irk7UaZ/VT/irSV9o5kapmvW3nbKXNlTVmfqGL9Q9Lc4VZRjEW3nbcbZfZT/d1qK8V+nCcBImJhOqb9PElbpfzt5utW3nbKXCk5QFunbALsTbHTqUwUO7BGkrcbZfZT/d1q6wOSdoiI2QAR8aSkd1GcGPXqEeTrVt52ylwpOUBbp1xM8XN0dnWFpGtGmLcbZfZT/d1q68EUp+3/TUQsBQ6W9N0R5OtW3nbKXCn5etBmZpnK9locZmarOgdoM7NMOUDbqJC0TNLsUpoygjLWl/TRzrfOLE8eg7ZRIenJiBi7gmVMAS6OiFe1+bjVI2LZitRt1gvuQVvPSFpd0kmSZkmaK+mItHyspKsk3ZquEzx0ze4Tga1TD/wkSbtLurhU3smSDk3TCyX9m6TrgAMkbS3pMkm3pOtmbzfaz9esXT7MzkbLOpKGDgG7OyLeAxwOPBYRO0laC/i1pCsoLqv5nijuJLMRcEO68cGxFKdk7wBQ46YBz0bErinvVcA/RcSdKu5U8y2KCxuZZcsB2kbLM0OBteTtwGsk7Z/mx1Ncw3gxcIKkt1Jcu3lzlj8bro4fQdEjp7it14+lv52AttYIyjMbVQ7Q1ksCjoyIy1+wsBimmAi8PiKek7SQ4oYCVUt54TBdNc9T6f9qwKMNviDMsuYxaOuly4F/lrQGgKRtJL2Ioif9UArOf0dx8wUo7gAzrvT4e4BXSFpL0niKy1IuJyIeB+6WdECqR5K2785TMuscB2jrpe9T3KH8VknzgO9S/Ko7C9hR0s3AByiuMU1EPEIxTj1P0klR3MX8XIq7vpxFcR3m4XwAOFzSHIprKU9rktcsCz7MzswsU+5Bm5llygHazCxTDtBmZplygDYzy5QDtJlZphygzcwy5QBtZpYpB2gzs0z9Lzj212gZECpQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = sns.heatmap(mean_matrix, annot=False, linewidths=0, \n",
    "            square = False, cmap = 'Blues_r');\n",
    "fig.set_ylim([0,2]);\n",
    "plt.ylabel('Group');\n",
    "plt.xlabel('Feature');\n",
    "all_sample_title = 'Feature Means by Group'\n",
    "plt.title(all_sample_title, size = 12);\n",
    "plt.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) (1 pt)** Based on the heat map, do you expect the feature variables to be useful for predicting spam versus nonspam? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, because it has different pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Train / test regularized logistic regression (10 pts)\n",
    "\n",
    "**a) (2 pts)** Split the data in the features matrix X and target y at random into training data and test data, using 10% of the data for testing. You should end up with two feature matrices (train and test) and two target arrays, the training y valaues and the test y values. Display the numbers of rows and columns of your train and test feature matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) (3 pts)** Fit an L1 penalized logistic regression model to your training data, using all of the features. Use an inverse penality C less than 1. If the model fit fails to converge try increasing the maximum number of iterations allowed. Display the feature coefficients for the training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "logitReg = LogisticRegression(penalty='l1', \n",
    "                              solver='liblinear', \n",
    "                              C=0.5, \n",
    "                              max_iter=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logitReg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) (3 pts)** Use the training model to get predicted categories (0/1) for the test data based on the test feature matrix, assuming a 0.5 cutoff for the predictive probability. Compute the overall accuracy (proportion correct) of these predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = logitReg.predict(X_test)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = logitReg.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) (2 pts)** Use the test data to compute the estimated sensitivy and specificity based on the confusion matrix, or adapting any functions we developed in class notes. Here we label 1 = positive (spam) and 0 = negative (not spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senspec(y, score, thresh, index=0):\n",
    "    yhat = 1*(score >= thresh)\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_true=y, \n",
    "                                              y_pred=yhat).ravel()\n",
    "    sens = tp / (fn + tp)\n",
    "    spec = tn / (fp + tn)\n",
    "    accuracy = (tn+tp)/(tn+fp+fn+tp)\n",
    "    return pd.DataFrame({'tn':[tn], \n",
    "                         'fp':[fp], \n",
    "                         'fn':[fn], \n",
    "                         'tp':[tp], \n",
    "                         'sens':[sens], \n",
    "                         'spec':[spec],\n",
    "                         'accuracy':[accuracy]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04348709, 0.03126508, 0.75693634, 0.99718675, 0.0892227 ,\n",
       "       0.8820183 , 0.08857469, 0.03723472, 0.18298787, 0.99998747,\n",
       "       0.99658645, 0.03126508, 0.21361399, 0.99535933, 0.99205493,\n",
       "       0.98101477, 0.29449702, 0.03440329, 0.04176871, 0.04293266,\n",
       "       0.03126508, 0.54940916, 0.97942701, 0.08502996, 0.7998192 ,\n",
       "       0.03351624, 0.05074588, 0.0753759 , 0.07029812, 0.98577186,\n",
       "       0.04297519, 1.        , 0.03758379, 0.99994113, 0.71012111,\n",
       "       0.97817843, 0.16087069, 0.03169774, 0.0560265 , 0.04272735])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phat01 = logitReg.predict_proba(X_test)\n",
    "phat1 = phat01[:,1]\n",
    "phat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "      <th>sens</th>\n",
       "      <th>spec</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tn  fp  fn  tp      sens      spec  accuracy\n",
       "0  23   3   1  13  0.928571  0.884615       0.9"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senspec(y_test, phat1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Cross-validation (5 pts)\n",
    "\n",
    "**a) (4 pts)** Using the same penalized logistic regression model you used in Part 2, use 10-fold cross validation (cv=10) to estimate the accuracy of the the L1 penalized logit classifier and also compute its standard error. Note that the estimated accuracy is the sample mean of the 10 different train/test accuracy scores produced by cross-validation, and the standard error for the mean is computed in the usual way for sample means. Display code and results. \n",
    "\n",
    "Note: if you have convergence problems, try using solver='liblinear' and max_iter=1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95 , 0.875, 0.85 , 0.9  , 0.825, 0.85 , 0.875, 0.825, 0.875,\n",
       "       0.925])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1 = cross_val_score(logitReg, X, y, cv=10)\n",
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % \\\n",
    "      (scores1.mean(), 2*scores1.std()/np.sqrt( 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) (1 pt)** Comment on whether the cross-validation average accuracy score differs noticeably from the single train/test accuracy estimate you calculated in Part 2. How would you explain the difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not significantly different. The differences cause by applying model to many train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
